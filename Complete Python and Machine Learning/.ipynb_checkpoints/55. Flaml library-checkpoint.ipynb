{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47b524da-9e7b-4a7d-8360-89dce70ea209",
   "metadata": {},
   "source": [
    "## Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a977f4e-6c38-4b40-93ff-04ad808dc72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-15 16:29:13] {1752} INFO - task = classification\n",
      "[flaml.automl.logger: 09-15 16:29:13] {1763} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 09-15 16:29:13] {1862} INFO - Minimizing error metric: 1-accuracy\n",
      "[flaml.automl.logger: 09-15 16:29:13] {1979} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'sgd', 'catboost', 'lrl1']\n",
      "[flaml.automl.logger: 09-15 16:29:13] {2282} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:29:14] {2417} INFO - Estimated sufficient time budget=3050s. Estimated necessary time budget=75s.\n",
      "[flaml.automl.logger: 09-15 16:29:14] {2466} INFO -  at 0.3s,\testimator lgbm's best error=0.0733,\tbest estimator lgbm's best error=0.0733\n",
      "[flaml.automl.logger: 09-15 16:29:14] {2282} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:29:14] {2466} INFO -  at 0.5s,\testimator lgbm's best error=0.0733,\tbest estimator lgbm's best error=0.0733\n",
      "[flaml.automl.logger: 09-15 16:29:14] {2282} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:29:14] {2466} INFO -  at 0.6s,\testimator lgbm's best error=0.0533,\tbest estimator lgbm's best error=0.0533\n",
      "[flaml.automl.logger: 09-15 16:29:14] {2282} INFO - iteration 3, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:29:14] {2466} INFO -  at 0.8s,\testimator lgbm's best error=0.0533,\tbest estimator lgbm's best error=0.0533\n",
      "[flaml.automl.logger: 09-15 16:29:14] {2282} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:29:14] {2466} INFO -  at 0.9s,\testimator lgbm's best error=0.0533,\tbest estimator lgbm's best error=0.0533\n",
      "[flaml.automl.logger: 09-15 16:29:14] {2282} INFO - iteration 5, current learner sgd\n",
      "[flaml.automl.logger: 09-15 16:29:14] {2466} INFO -  at 1.1s,\testimator sgd's best error=0.3333,\tbest estimator lgbm's best error=0.0533\n",
      "[flaml.automl.logger: 09-15 16:29:14] {2282} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:29:15] {2466} INFO -  at 1.2s,\testimator lgbm's best error=0.0467,\tbest estimator lgbm's best error=0.0467\n",
      "[flaml.automl.logger: 09-15 16:29:15] {2282} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:29:15] {2466} INFO -  at 1.4s,\testimator lgbm's best error=0.0467,\tbest estimator lgbm's best error=0.0467\n",
      "[flaml.automl.logger: 09-15 16:29:15] {2282} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:29:15] {2466} INFO -  at 1.5s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:15] {2282} INFO - iteration 9, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:29:15] {2466} INFO -  at 1.7s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:15] {2282} INFO - iteration 10, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:29:15] {2466} INFO -  at 1.8s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:15] {2282} INFO - iteration 11, current learner sgd\n",
      "[flaml.automl.logger: 09-15 16:29:15] {2466} INFO -  at 2.0s,\testimator sgd's best error=0.1200,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:15] {2282} INFO - iteration 12, current learner sgd\n",
      "[flaml.automl.logger: 09-15 16:29:16] {2466} INFO -  at 2.3s,\testimator sgd's best error=0.1200,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:16] {2282} INFO - iteration 13, current learner sgd\n",
      "[flaml.automl.logger: 09-15 16:29:16] {2466} INFO -  at 2.4s,\testimator sgd's best error=0.1133,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:16] {2282} INFO - iteration 14, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:29:16] {2466} INFO -  at 2.7s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:16] {2282} INFO - iteration 15, current learner xgboost\n",
      "[flaml.automl.logger: 09-15 16:29:16] {2466} INFO -  at 2.9s,\testimator xgboost's best error=0.0600,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:16] {2282} INFO - iteration 16, current learner xgboost\n",
      "[flaml.automl.logger: 09-15 16:29:16] {2466} INFO -  at 3.1s,\testimator xgboost's best error=0.0600,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:16] {2282} INFO - iteration 17, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:29:17] {2466} INFO -  at 3.3s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:17] {2282} INFO - iteration 18, current learner xgboost\n",
      "[flaml.automl.logger: 09-15 16:29:17] {2466} INFO -  at 3.4s,\testimator xgboost's best error=0.0600,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:17] {2282} INFO - iteration 19, current learner xgboost\n",
      "[flaml.automl.logger: 09-15 16:29:17] {2466} INFO -  at 3.5s,\testimator xgboost's best error=0.0600,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:17] {2282} INFO - iteration 20, current learner xgboost\n",
      "[flaml.automl.logger: 09-15 16:29:17] {2466} INFO -  at 3.7s,\testimator xgboost's best error=0.0600,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:17] {2282} INFO - iteration 21, current learner extra_tree\n",
      "[flaml.automl.logger: 09-15 16:29:18] {2466} INFO -  at 4.2s,\testimator extra_tree's best error=0.1533,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:18] {2282} INFO - iteration 22, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:29:18] {2466} INFO -  at 4.4s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:18] {2282} INFO - iteration 23, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:29:18] {2466} INFO -  at 4.6s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:18] {2282} INFO - iteration 24, current learner extra_tree\n",
      "[flaml.automl.logger: 09-15 16:29:18] {2466} INFO -  at 4.9s,\testimator extra_tree's best error=0.1533,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:18] {2282} INFO - iteration 25, current learner sgd\n",
      "[flaml.automl.logger: 09-15 16:29:18] {2466} INFO -  at 5.1s,\testimator sgd's best error=0.1133,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:18] {2282} INFO - iteration 26, current learner xgboost\n",
      "[flaml.automl.logger: 09-15 16:29:19] {2466} INFO -  at 5.3s,\testimator xgboost's best error=0.0600,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:19] {2282} INFO - iteration 27, current learner rf\n",
      "[flaml.automl.logger: 09-15 16:29:19] {2466} INFO -  at 5.8s,\testimator rf's best error=0.0867,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:19] {2282} INFO - iteration 28, current learner xgboost\n",
      "[flaml.automl.logger: 09-15 16:29:19] {2466} INFO -  at 6.0s,\testimator xgboost's best error=0.0600,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:19] {2282} INFO - iteration 29, current learner extra_tree\n",
      "[flaml.automl.logger: 09-15 16:29:20] {2466} INFO -  at 6.4s,\testimator extra_tree's best error=0.0733,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:20] {2282} INFO - iteration 30, current learner extra_tree\n",
      "[flaml.automl.logger: 09-15 16:29:20] {2466} INFO -  at 7.0s,\testimator extra_tree's best error=0.0733,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:20] {2282} INFO - iteration 31, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:29:21] {2466} INFO -  at 7.2s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:21] {2282} INFO - iteration 32, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:29:21] {2466} INFO -  at 7.3s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:21] {2282} INFO - iteration 33, current learner rf\n",
      "[flaml.automl.logger: 09-15 16:29:21] {2466} INFO -  at 7.7s,\testimator rf's best error=0.0733,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:21] {2282} INFO - iteration 34, current learner extra_tree\n",
      "[flaml.automl.logger: 09-15 16:29:21] {2466} INFO -  at 8.1s,\testimator extra_tree's best error=0.0733,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:21] {2282} INFO - iteration 35, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:29:22] {2466} INFO -  at 8.2s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:22] {2282} INFO - iteration 36, current learner xgboost\n",
      "[flaml.automl.logger: 09-15 16:29:22] {2466} INFO -  at 8.4s,\testimator xgboost's best error=0.0533,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:22] {2282} INFO - iteration 37, current learner extra_tree\n",
      "[flaml.automl.logger: 09-15 16:29:22] {2466} INFO -  at 9.1s,\testimator extra_tree's best error=0.0733,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:22] {2282} INFO - iteration 38, current learner rf\n",
      "[flaml.automl.logger: 09-15 16:29:23] {2466} INFO -  at 9.7s,\testimator rf's best error=0.0667,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:23] {2282} INFO - iteration 39, current learner xgboost\n",
      "[flaml.automl.logger: 09-15 16:29:23] {2466} INFO -  at 9.8s,\testimator xgboost's best error=0.0533,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:23] {2282} INFO - iteration 40, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:29:23] {2466} INFO -  at 9.9s,\testimator lgbm's best error=0.0400,\tbest estimator lgbm's best error=0.0400\n",
      "[flaml.automl.logger: 09-15 16:29:23] {2282} INFO - iteration 41, current learner catboost\n"
     ]
    }
   ],
   "source": [
    "from flaml import AutoML\n",
    "from sklearn.datasets import load_iris\n",
    "# Initialize an AutoML instance\n",
    "automl = AutoML()\n",
    "# Specify automl goal and constraint\n",
    "automl_settings = {\n",
    "    \"time_budget\": 10,  # in seconds\n",
    "    \"metric\": 'accuracy',\n",
    "    \"task\": 'classification',\n",
    "    \"log_file_name\": \"cancer.log\",\n",
    "}\n",
    "X_train, y_train = load_iris(return_X_y=True)\n",
    "# Train with labeled input data\n",
    "automl.fit(X_train=X_train, y_train=y_train,\n",
    "           **automl_settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e812d5-d056-4bee-b4d7-f5bbeea2f498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.81927853 0.15011003 0.03061144]\n",
      " [0.7725578  0.20229926 0.02514294]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.94986785 0.0227757  0.02735645]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.81927853 0.15011003 0.03061144]\n",
      " [0.7725578  0.20229926 0.02514294]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.95482095 0.02289446 0.02228459]\n",
      " [0.03532049 0.90765492 0.05702459]\n",
      " [0.02426335 0.94561863 0.03011802]\n",
      " [0.02979822 0.76574527 0.20445651]\n",
      " [0.02425689 0.94536674 0.03037637]\n",
      " [0.02426335 0.94561863 0.03011802]\n",
      " [0.02439742 0.95084371 0.02475887]\n",
      " [0.04597815 0.88593955 0.0680823 ]\n",
      " [0.03444868 0.93549223 0.03005908]\n",
      " [0.02439742 0.95084371 0.02475887]\n",
      " [0.02439742 0.95084371 0.02475887]\n",
      " [0.03444868 0.93549223 0.03005908]\n",
      " [0.02426335 0.94561863 0.03011802]\n",
      " [0.03444868 0.93549223 0.03005908]\n",
      " [0.03532049 0.90765492 0.05702459]\n",
      " [0.02439742 0.95084371 0.02475887]\n",
      " [0.02439742 0.95084371 0.02475887]\n",
      " [0.02426335 0.94561863 0.03011802]\n",
      " [0.03464616 0.94085506 0.02449878]\n",
      " [0.02409355 0.93900101 0.03690544]\n",
      " [0.02425689 0.94536674 0.03037637]\n",
      " [0.05607781 0.22602748 0.71789471]\n",
      " [0.02439742 0.95084371 0.02475887]\n",
      " [0.02979822 0.76574527 0.20445651]\n",
      " [0.03532049 0.90765492 0.05702459]\n",
      " [0.02439742 0.95084371 0.02475887]\n",
      " [0.02439742 0.95084371 0.02475887]\n",
      " [0.03421511 0.8792491  0.08653579]\n",
      " [0.04750441 0.10416041 0.84833518]\n",
      " [0.02426335 0.94561863 0.03011802]\n",
      " [0.03464616 0.94085506 0.02449878]\n",
      " [0.02425689 0.94536674 0.03037637]\n",
      " [0.03444868 0.93549223 0.03005908]\n",
      " [0.02439742 0.95084371 0.02475887]\n",
      " [0.05448352 0.24041868 0.70509779]\n",
      " [0.02426335 0.94561863 0.03011802]\n",
      " [0.03209893 0.93802491 0.02987616]\n",
      " [0.03487663 0.89624882 0.06887455]\n",
      " [0.02425689 0.94536674 0.03037637]\n",
      " [0.02439742 0.95084371 0.02475887]\n",
      " [0.02425689 0.94536674 0.03037637]\n",
      " [0.02439742 0.95084371 0.02475887]\n",
      " [0.02439742 0.95084371 0.02475887]\n",
      " [0.02439742 0.95084371 0.02475887]\n",
      " [0.03444868 0.93549223 0.03005908]\n",
      " [0.02439742 0.95084371 0.02475887]\n",
      " [0.02439742 0.95084371 0.02475887]\n",
      " [0.02439742 0.95084371 0.02475887]\n",
      " [0.02439742 0.95084371 0.02475887]\n",
      " [0.20190513 0.77324896 0.02484591]\n",
      " [0.02439742 0.95084371 0.02475887]\n",
      " [0.02913985 0.02871742 0.94214272]\n",
      " [0.02748767 0.04665313 0.9258592 ]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.06616033 0.26666613 0.66717354]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.02913985 0.02871742 0.94214272]\n",
      " [0.02748767 0.04665313 0.9258592 ]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.04750441 0.10416041 0.84833518]\n",
      " [0.02748767 0.04665313 0.9258592 ]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.02913985 0.02871742 0.94214272]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.06077965 0.52218884 0.41703151]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.0436898  0.17609634 0.78021386]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.0436898  0.17609634 0.78021386]\n",
      " [0.02913985 0.02871742 0.94214272]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.05607781 0.22602748 0.71789471]\n",
      " [0.0436898  0.17609634 0.78021386]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.04765475 0.16284178 0.78950347]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.02913985 0.02871742 0.94214272]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.05448352 0.24041868 0.70509779]\n",
      " [0.04765475 0.16284178 0.78950347]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.02913985 0.02871742 0.94214272]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.05607781 0.22602748 0.71789471]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.02748767 0.04665313 0.9258592 ]\n",
      " [0.02748767 0.04665313 0.9258592 ]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.02913985 0.02871742 0.94214272]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.04750441 0.10416041 0.84833518]\n",
      " [0.02201018 0.02892832 0.9490615 ]\n",
      " [0.02913985 0.02871742 0.94214272]\n",
      " [0.02748767 0.04665313 0.9258592 ]]\n",
      "<flaml.automl.model.LGBMEstimator object at 0x00000250EEAA9420>\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "print(automl.predict_proba(X_train))\n",
    "# Export the best model\n",
    "print(automl.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c704054-bf07-40db-a146-b2b4ff46ab71",
   "metadata": {},
   "source": [
    "## Regression Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "137bfa73-a7ad-461d-9ac4-90f3a033b347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 09-15 16:26:03] {1752} INFO - task = regression\n",
      "[flaml.automl.logger: 09-15 16:26:03] {1763} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 09-15 16:26:03] {1862} INFO - Minimizing error metric: 1-r2\n",
      "[flaml.automl.logger: 09-15 16:26:03] {1979} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'sgd', 'catboost']\n",
      "[flaml.automl.logger: 09-15 16:26:03] {2282} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:26:03] {2417} INFO - Estimated sufficient time budget=1530s. Estimated necessary time budget=13s.\n",
      "[flaml.automl.logger: 09-15 16:26:03] {2466} INFO -  at 0.2s,\testimator lgbm's best error=0.5603,\tbest estimator lgbm's best error=0.5603\n",
      "[flaml.automl.logger: 09-15 16:26:03] {2282} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:26:03] {2466} INFO -  at 0.3s,\testimator lgbm's best error=0.5603,\tbest estimator lgbm's best error=0.5603\n",
      "[flaml.automl.logger: 09-15 16:26:03] {2282} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:26:03] {2466} INFO -  at 0.4s,\testimator lgbm's best error=0.2770,\tbest estimator lgbm's best error=0.2770\n",
      "[flaml.automl.logger: 09-15 16:26:03] {2282} INFO - iteration 3, current learner sgd\n",
      "[flaml.automl.logger: 09-15 16:26:04] {2466} INFO -  at 0.7s,\testimator sgd's best error=0.5203,\tbest estimator lgbm's best error=0.2770\n",
      "[flaml.automl.logger: 09-15 16:26:04] {2282} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:26:04] {2466} INFO -  at 0.9s,\testimator lgbm's best error=0.2233,\tbest estimator lgbm's best error=0.2233\n",
      "[flaml.automl.logger: 09-15 16:26:04] {2282} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:26:04] {2466} INFO -  at 1.1s,\testimator lgbm's best error=0.2233,\tbest estimator lgbm's best error=0.2233\n",
      "[flaml.automl.logger: 09-15 16:26:04] {2282} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:26:04] {2466} INFO -  at 1.3s,\testimator lgbm's best error=0.2113,\tbest estimator lgbm's best error=0.2113\n",
      "[flaml.automl.logger: 09-15 16:26:04] {2282} INFO - iteration 7, current learner xgboost\n",
      "[flaml.automl.logger: 09-15 16:26:04] {2466} INFO -  at 1.5s,\testimator xgboost's best error=0.5600,\tbest estimator lgbm's best error=0.2113\n",
      "[flaml.automl.logger: 09-15 16:26:04] {2282} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:26:05] {2466} INFO -  at 1.7s,\testimator lgbm's best error=0.2113,\tbest estimator lgbm's best error=0.2113\n",
      "[flaml.automl.logger: 09-15 16:26:05] {2282} INFO - iteration 9, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:26:05] {2466} INFO -  at 1.8s,\testimator lgbm's best error=0.2113,\tbest estimator lgbm's best error=0.2113\n",
      "[flaml.automl.logger: 09-15 16:26:05] {2282} INFO - iteration 10, current learner xgboost\n",
      "[flaml.automl.logger: 09-15 16:26:05] {2466} INFO -  at 2.0s,\testimator xgboost's best error=0.5600,\tbest estimator lgbm's best error=0.2113\n",
      "[flaml.automl.logger: 09-15 16:26:05] {2282} INFO - iteration 11, current learner xgboost\n",
      "[flaml.automl.logger: 09-15 16:26:05] {2466} INFO -  at 2.1s,\testimator xgboost's best error=0.2791,\tbest estimator lgbm's best error=0.2113\n",
      "[flaml.automl.logger: 09-15 16:26:05] {2282} INFO - iteration 12, current learner xgboost\n",
      "[flaml.automl.logger: 09-15 16:26:05] {2466} INFO -  at 2.3s,\testimator xgboost's best error=0.2791,\tbest estimator lgbm's best error=0.2113\n",
      "[flaml.automl.logger: 09-15 16:26:05] {2282} INFO - iteration 13, current learner xgboost\n",
      "[flaml.automl.logger: 09-15 16:26:06] {2466} INFO -  at 2.5s,\testimator xgboost's best error=0.2791,\tbest estimator lgbm's best error=0.2113\n",
      "[flaml.automl.logger: 09-15 16:26:06] {2282} INFO - iteration 14, current learner sgd\n",
      "[flaml.automl.logger: 09-15 16:26:06] {2466} INFO -  at 2.6s,\testimator sgd's best error=0.5203,\tbest estimator lgbm's best error=0.2113\n",
      "[flaml.automl.logger: 09-15 16:26:06] {2282} INFO - iteration 15, current learner extra_tree\n",
      "[flaml.automl.logger: 09-15 16:26:06] {2466} INFO -  at 3.0s,\testimator extra_tree's best error=0.1897,\tbest estimator extra_tree's best error=0.1897\n",
      "[flaml.automl.logger: 09-15 16:26:06] {2282} INFO - iteration 16, current learner rf\n",
      "[flaml.automl.logger: 09-15 16:26:06] {2466} INFO -  at 3.4s,\testimator rf's best error=0.1757,\tbest estimator rf's best error=0.1757\n",
      "[flaml.automl.logger: 09-15 16:26:06] {2282} INFO - iteration 17, current learner rf\n",
      "[flaml.automl.logger: 09-15 16:26:07] {2466} INFO -  at 3.8s,\testimator rf's best error=0.1637,\tbest estimator rf's best error=0.1637\n",
      "[flaml.automl.logger: 09-15 16:26:07] {2282} INFO - iteration 18, current learner sgd\n",
      "[flaml.automl.logger: 09-15 16:26:07] {2466} INFO -  at 3.8s,\testimator sgd's best error=0.5203,\tbest estimator rf's best error=0.1637\n",
      "[flaml.automl.logger: 09-15 16:26:07] {2282} INFO - iteration 19, current learner rf\n",
      "[flaml.automl.logger: 09-15 16:26:07] {2466} INFO -  at 4.2s,\testimator rf's best error=0.1618,\tbest estimator rf's best error=0.1618\n",
      "[flaml.automl.logger: 09-15 16:26:07] {2282} INFO - iteration 20, current learner extra_tree\n",
      "[flaml.automl.logger: 09-15 16:26:08] {2466} INFO -  at 4.5s,\testimator extra_tree's best error=0.1611,\tbest estimator extra_tree's best error=0.1611\n",
      "[flaml.automl.logger: 09-15 16:26:08] {2282} INFO - iteration 21, current learner xgboost\n",
      "[flaml.automl.logger: 09-15 16:26:08] {2466} INFO -  at 4.7s,\testimator xgboost's best error=0.2791,\tbest estimator extra_tree's best error=0.1611\n",
      "[flaml.automl.logger: 09-15 16:26:08] {2282} INFO - iteration 22, current learner extra_tree\n",
      "[flaml.automl.logger: 09-15 16:26:08] {2466} INFO -  at 5.1s,\testimator extra_tree's best error=0.1611,\tbest estimator extra_tree's best error=0.1611\n",
      "[flaml.automl.logger: 09-15 16:26:08] {2282} INFO - iteration 23, current learner rf\n",
      "[flaml.automl.logger: 09-15 16:26:09] {2466} INFO -  at 5.8s,\testimator rf's best error=0.1618,\tbest estimator extra_tree's best error=0.1611\n",
      "[flaml.automl.logger: 09-15 16:26:09] {2282} INFO - iteration 24, current learner xgboost\n",
      "[flaml.automl.logger: 09-15 16:26:09] {2466} INFO -  at 6.1s,\testimator xgboost's best error=0.2061,\tbest estimator extra_tree's best error=0.1611\n",
      "[flaml.automl.logger: 09-15 16:26:09] {2282} INFO - iteration 25, current learner sgd\n",
      "[flaml.automl.logger: 09-15 16:26:09] {2466} INFO -  at 6.1s,\testimator sgd's best error=0.5203,\tbest estimator extra_tree's best error=0.1611\n",
      "[flaml.automl.logger: 09-15 16:26:09] {2282} INFO - iteration 26, current learner sgd\n",
      "[flaml.automl.logger: 09-15 16:26:09] {2466} INFO -  at 6.2s,\testimator sgd's best error=0.4666,\tbest estimator extra_tree's best error=0.1611\n",
      "[flaml.automl.logger: 09-15 16:26:09] {2282} INFO - iteration 27, current learner xgboost\n",
      "[flaml.automl.logger: 09-15 16:26:10] {2466} INFO -  at 6.5s,\testimator xgboost's best error=0.2061,\tbest estimator extra_tree's best error=0.1611\n",
      "[flaml.automl.logger: 09-15 16:26:10] {2282} INFO - iteration 28, current learner extra_tree\n",
      "[flaml.automl.logger: 09-15 16:26:10] {2466} INFO -  at 7.2s,\testimator extra_tree's best error=0.1440,\tbest estimator extra_tree's best error=0.1440\n",
      "[flaml.automl.logger: 09-15 16:26:10] {2282} INFO - iteration 29, current learner sgd\n",
      "[flaml.automl.logger: 09-15 16:26:10] {2466} INFO -  at 7.3s,\testimator sgd's best error=0.4516,\tbest estimator extra_tree's best error=0.1440\n",
      "[flaml.automl.logger: 09-15 16:26:10] {2282} INFO - iteration 30, current learner sgd\n",
      "[flaml.automl.logger: 09-15 16:26:10] {2466} INFO -  at 7.4s,\testimator sgd's best error=0.4516,\tbest estimator extra_tree's best error=0.1440\n",
      "[flaml.automl.logger: 09-15 16:26:10] {2282} INFO - iteration 31, current learner rf\n",
      "[flaml.automl.logger: 09-15 16:26:11] {2466} INFO -  at 7.9s,\testimator rf's best error=0.1618,\tbest estimator extra_tree's best error=0.1440\n",
      "[flaml.automl.logger: 09-15 16:26:11] {2282} INFO - iteration 32, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:26:11] {2466} INFO -  at 8.4s,\testimator lgbm's best error=0.1612,\tbest estimator extra_tree's best error=0.1440\n",
      "[flaml.automl.logger: 09-15 16:26:11] {2282} INFO - iteration 33, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:26:12] {2466} INFO -  at 8.6s,\testimator lgbm's best error=0.1612,\tbest estimator extra_tree's best error=0.1440\n",
      "[flaml.automl.logger: 09-15 16:26:12] {2282} INFO - iteration 34, current learner extra_tree\n",
      "[flaml.automl.logger: 09-15 16:26:12] {2466} INFO -  at 9.3s,\testimator extra_tree's best error=0.1402,\tbest estimator extra_tree's best error=0.1402\n",
      "[flaml.automl.logger: 09-15 16:26:12] {2282} INFO - iteration 35, current learner lgbm\n",
      "[flaml.automl.logger: 09-15 16:26:13] {2466} INFO -  at 10.0s,\testimator lgbm's best error=0.1492,\tbest estimator extra_tree's best error=0.1402\n",
      "[flaml.automl.logger: 09-15 16:26:13] {2724} INFO - retrain extra_tree for 0.0s\n",
      "[flaml.automl.logger: 09-15 16:26:13] {2727} INFO - retrained model: ExtraTreesRegressor(max_features=0.7754448469593765, max_leaf_nodes=29,\n",
      "                    n_estimators=5, n_jobs=-1, random_state=12032022)\n",
      "[flaml.automl.logger: 09-15 16:26:13] {2009} INFO - fit succeeded\n",
      "[flaml.automl.logger: 09-15 16:26:13] {2010} INFO - Time taken to find the best model: 9.268839836120605\n",
      "[0.         0.00740741 0.         0.00357143 0.         0.00357143\n",
      " 0.         0.00357143 0.00357143 0.00357143 0.00740741 0.01097884\n",
      " 0.         0.         0.00357143 0.00357143 0.00740741 0.00357143\n",
      " 0.         0.99133555 0.99397357 0.99233422 0.00357143 0.\n",
      " 0.00357143 0.         0.00357143 0.         0.00357143 0.00740741\n",
      " 0.         0.00357143 0.         0.         0.00357143 0.\n",
      " 0.00357143 0.99233422 0.         0.00357143 0.57818433 0.04\n",
      " 0.         0.00357143 0.00357143 0.         0.99233422 0.00357143\n",
      " 0.99233422 0.97751987 0.99342118 0.99559959 0.99233422 0.\n",
      " 0.19735099 0.99342118 0.         0.00357143 0.99342118 0.99233422\n",
      " 0.98433027 0.99233422 0.00357143 0.99342118 0.00357143 0.\n",
      " 0.99233422 0.99233422 0.98340131 0.99233422 0.         0.99342118\n",
      " 0.         0.13333333 0.99233422 0.00740741 0.99133555 0.\n",
      " 0.         0.99233422 0.99342118 0.7902381  0.         0.\n",
      " 0.99233422 0.         0.04       0.         0.99233422 0.992\n",
      " 0.97576008 0.         0.98662074 0.93001987 0.         0.\n",
      " 0.99342118 0.99233422 0.99233422 0.00357143 0.         0.99233422\n",
      " 0.99233422 0.99233422 0.99342118 0.00357143 0.99233422 0.99233422\n",
      " 0.         0.99233422 0.99342118 0.97800221 1.         0.99233422\n",
      " 0.99233422 0.99342118 0.99233422 0.00357143 0.00357143 0.00740741\n",
      " 0.99233422 0.         0.         0.99133555 0.99397357 0.99001987\n",
      " 0.19735099 0.00740741 0.992      0.         0.99233422 0.\n",
      " 0.00740741 1.         0.         0.99233422 0.98433027 0.99233422\n",
      " 0.         0.99133555 0.99233422 0.00740741 0.99233422 0.99233422\n",
      " 0.99233422 0.99342118 0.00357143 0.99090909 0.99333333 0.99001987\n",
      " 0.98433027 0.98008785 1.         0.99233422 0.99668654 0.99233422\n",
      " 0.         1.         0.99233422 0.99233422 0.98697928 0.\n",
      " 0.         0.99233422 0.         0.99068433 0.99233422 0.00740741\n",
      " 0.         0.99735099 0.99233422 0.         0.00357143 0.99233422\n",
      " 0.99233422 0.99233422 0.99342118 0.00357143 0.99233422 0.99233422\n",
      " 0.         0.         0.00740741 0.99342118 0.         0.99342118\n",
      " 0.00740741 0.99233422 0.99233422 0.99233422 0.00357143 0.92097706\n",
      " 0.99233422 0.         0.         0.99233422 0.00357143 0.\n",
      " 0.         0.00357143 0.99342118 0.00740741 0.         0.00357143\n",
      " 0.9950472  0.23333333 0.99233422 0.00740741 0.83824859 1.\n",
      " 0.         0.99233422 0.         0.         0.00357143 0.00357143\n",
      " 0.99233422 0.99342118 0.         0.         0.99001987 0.99266888\n",
      " 0.99233422 0.00357143 0.9950472  0.99090909 0.99233422 0.99068433\n",
      " 0.99233422 0.00357143 0.         0.99233422 0.99233422 0.\n",
      " 0.99233422 0.97751987 0.         0.00740741 0.9875     0.\n",
      " 0.99668654 0.99233422 0.98008785 0.99024464 0.         0.99342118\n",
      " 0.99233422 0.9807523  0.99233422 0.99233422 0.         0.99342118\n",
      " 0.         0.         0.         0.192      0.         0.00357143\n",
      " 0.00357143 0.00357143 0.         0.00740741 0.00740741 0.\n",
      " 0.00740741 0.         0.99342118 0.98418654 0.99233422 0.99233422\n",
      " 0.99001987 0.99233422 0.         0.99233422 0.00740741 0.91624242\n",
      " 0.99233422 0.00740741 0.99001987 0.99559959 0.         0.98433027\n",
      " 0.         0.00357143 0.99342118 0.99342118 0.99233422 0.99233422\n",
      " 0.98224464 0.99233422 0.98290909 1.         0.99233422 0.99233422\n",
      " 0.99233422 0.99559959 0.99233422 0.72097706 0.99068433 0.99233422\n",
      " 0.         0.99342118 0.         0.99233422 0.99342118 0.99233422\n",
      " 0.99233422 0.99233422 0.99001987 0.9950472  0.99233422 0.99735099\n",
      " 0.99342118 0.99233422 0.99233422 0.99233422 0.99233422 0.\n",
      " 0.99133555 0.99342118 0.99342118 0.         0.99233422 0.\n",
      " 0.99233422 0.99233422 0.99668654 0.99233422 0.         0.\n",
      " 0.         0.99233422 0.99233422 0.99233422 0.99233422 0.\n",
      " 0.99233422 0.00740741 0.99233422 0.         0.99735099 0.99233422\n",
      " 0.99233422 0.         0.99342118 0.99233422 0.99233422 0.99735099\n",
      " 0.99233422 0.99342118 0.99342118 0.00357143 0.         0.00357143\n",
      " 0.99342118 0.99607019 0.97800221 0.99001987 0.99342118 0.99342118\n",
      " 0.99342118 0.98147357 0.99233422 1.         0.99559959 0.00740741\n",
      " 0.         0.99233422 0.         0.         0.00357143 0.99068433\n",
      " 0.         0.         0.99559959 0.99333333 0.98666667 0.97751987\n",
      " 0.99397357 0.00357143 0.99024859 0.99233422 0.99233422 0.99233422\n",
      " 0.99397357 0.04       0.99233422 0.99668654 0.99342118 0.\n",
      " 0.99233422 0.99342118 0.00357143 0.         0.99233422 0.99001987\n",
      " 0.99333333 0.99342118 0.99233422 0.99342118 0.         0.99233422\n",
      " 0.99233422 0.99233422 0.99342118 0.99342118 1.         0.98697928\n",
      " 0.         0.99342118 0.99233422 0.99233422 0.99233422 0.98485099\n",
      " 0.         0.99233422 0.99342118 0.00357143 0.99233422 0.99233422\n",
      " 0.99233422 0.9        0.99662258 0.97866667 0.98697928 0.99233422\n",
      " 0.99233422 0.99233422 0.99233422 0.99342118 0.00357143 0.99233422\n",
      " 0.         0.         0.99735099 0.00357143 0.99342118 0.99668654\n",
      " 0.99668654 0.99001987 0.9807523  0.         0.99668654 0.99233422\n",
      " 0.         0.99233422 0.         0.99068433 0.98485099 0.\n",
      " 0.99233422 0.         0.99233422 0.92466888 0.99233422 0.98418654\n",
      " 0.99342118 0.99233422 0.99233422 0.99233422 0.00740741 0.\n",
      " 0.97751987 0.99233422 0.98838053 0.99668654 0.99668654 0.99233422\n",
      " 0.         0.992      0.99342118 0.98697928 0.99735099 0.98433027\n",
      " 0.99233422 0.99233422 0.99735099 0.99001987 0.99233422 0.00357143\n",
      " 0.99233422 0.99735099 0.99024859 0.99001987 1.         0.99133555\n",
      " 0.99068433 0.         0.99342118 0.00740741 0.99233422 0.80740741\n",
      " 0.         0.99342118 0.99342118 0.98533333 0.99091304 0.99233422\n",
      " 0.         0.         0.99090909 0.00357143 0.99233422 0.\n",
      " 0.97800221 0.97800221 0.99233422 0.99233422 1.         0.00357143\n",
      " 0.99233422 0.99068433 0.00357143 0.99090909 0.         0.99233422\n",
      " 0.         0.         0.99133555 0.99342118 0.99233422 0.\n",
      " 0.99233422 0.93001987 0.99233422 0.99233422 0.99735099 0.99233422\n",
      " 0.98224464 0.99233422 0.99607019 0.99233422 0.99068433 0.\n",
      " 0.99233422 0.         0.22666667 0.99662258 0.99233422 0.99233422\n",
      " 0.99233422 0.91818433 0.98485099 0.99233422 0.99668654 0.98418654\n",
      " 0.99233422 0.99233422 0.99233422 0.99342118 0.99233422 0.99233422\n",
      " 0.99233422 0.99233422 0.99233422 0.99233422 0.99233422 0.99342118\n",
      " 0.98335321 0.98008785 0.98418654 0.99342118 0.00357143 0.\n",
      " 0.         0.         0.00740741 0.         0.99233422]\n",
      "<flaml.automl.model.ExtraTreesEstimator object at 0x00000250EED773A0>\n"
     ]
    }
   ],
   "source": [
    "## Regression Problem\n",
    "from flaml import AutoML\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "# Initialize an AutoML instance\n",
    "automl = AutoML()\n",
    "# Specify automl goal and constraint\n",
    "automl_settings = {\n",
    "    \"time_budget\": 10,  # in seconds\n",
    "    \"metric\": 'r2',\n",
    "    \"task\": 'regression',\n",
    "    \"log_file_name\": \"boston.log\",\n",
    "}\n",
    "X_train, y_train = load_breast_cancer(return_X_y=True)\n",
    "# Train with labeled input data\n",
    "automl.fit(X_train=X_train, y_train=y_train,\n",
    "           **automl_settings)\n",
    "# Predict\n",
    "print(automl.predict(X_train))\n",
    "# Export the best model\n",
    "print(automl.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f19f68a-8810-4a59-9433-7578775febb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.00740741 0.         0.00357143 0.         0.00357143\n",
      " 0.         0.00357143 0.00357143 0.00357143 0.00740741 0.01097884\n",
      " 0.         0.         0.00357143 0.00357143 0.00740741 0.00357143\n",
      " 0.         0.99133555 0.99397357 0.99233422 0.00357143 0.\n",
      " 0.00357143 0.         0.00357143 0.         0.00357143 0.00740741\n",
      " 0.         0.00357143 0.         0.         0.00357143 0.\n",
      " 0.00357143 0.99233422 0.         0.00357143 0.57818433 0.04\n",
      " 0.         0.00357143 0.00357143 0.         0.99233422 0.00357143\n",
      " 0.99233422 0.97751987 0.99342118 0.99559959 0.99233422 0.\n",
      " 0.19735099 0.99342118 0.         0.00357143 0.99342118 0.99233422\n",
      " 0.98433027 0.99233422 0.00357143 0.99342118 0.00357143 0.\n",
      " 0.99233422 0.99233422 0.98340131 0.99233422 0.         0.99342118\n",
      " 0.         0.13333333 0.99233422 0.00740741 0.99133555 0.\n",
      " 0.         0.99233422 0.99342118 0.7902381  0.         0.\n",
      " 0.99233422 0.         0.04       0.         0.99233422 0.992\n",
      " 0.97576008 0.         0.98662074 0.93001987 0.         0.\n",
      " 0.99342118 0.99233422 0.99233422 0.00357143 0.         0.99233422\n",
      " 0.99233422 0.99233422 0.99342118 0.00357143 0.99233422 0.99233422\n",
      " 0.         0.99233422 0.99342118 0.97800221 1.         0.99233422\n",
      " 0.99233422 0.99342118 0.99233422 0.00357143 0.00357143 0.00740741\n",
      " 0.99233422 0.         0.         0.99133555 0.99397357 0.99001987\n",
      " 0.19735099 0.00740741 0.992      0.         0.99233422 0.\n",
      " 0.00740741 1.         0.         0.99233422 0.98433027 0.99233422\n",
      " 0.         0.99133555 0.99233422 0.00740741 0.99233422 0.99233422\n",
      " 0.99233422 0.99342118 0.00357143 0.99090909 0.99333333 0.99001987\n",
      " 0.98433027 0.98008785 1.         0.99233422 0.99668654 0.99233422\n",
      " 0.         1.         0.99233422 0.99233422 0.98697928 0.\n",
      " 0.         0.99233422 0.         0.99068433 0.99233422 0.00740741\n",
      " 0.         0.99735099 0.99233422 0.         0.00357143 0.99233422\n",
      " 0.99233422 0.99233422 0.99342118 0.00357143 0.99233422 0.99233422\n",
      " 0.         0.         0.00740741 0.99342118 0.         0.99342118\n",
      " 0.00740741 0.99233422 0.99233422 0.99233422 0.00357143 0.92097706\n",
      " 0.99233422 0.         0.         0.99233422 0.00357143 0.\n",
      " 0.         0.00357143 0.99342118 0.00740741 0.         0.00357143\n",
      " 0.9950472  0.23333333 0.99233422 0.00740741 0.83824859 1.\n",
      " 0.         0.99233422 0.         0.         0.00357143 0.00357143\n",
      " 0.99233422 0.99342118 0.         0.         0.99001987 0.99266888\n",
      " 0.99233422 0.00357143 0.9950472  0.99090909 0.99233422 0.99068433\n",
      " 0.99233422 0.00357143 0.         0.99233422 0.99233422 0.\n",
      " 0.99233422 0.97751987 0.         0.00740741 0.9875     0.\n",
      " 0.99668654 0.99233422 0.98008785 0.99024464 0.         0.99342118\n",
      " 0.99233422 0.9807523  0.99233422 0.99233422 0.         0.99342118\n",
      " 0.         0.         0.         0.192      0.         0.00357143\n",
      " 0.00357143 0.00357143 0.         0.00740741 0.00740741 0.\n",
      " 0.00740741 0.         0.99342118 0.98418654 0.99233422 0.99233422\n",
      " 0.99001987 0.99233422 0.         0.99233422 0.00740741 0.91624242\n",
      " 0.99233422 0.00740741 0.99001987 0.99559959 0.         0.98433027\n",
      " 0.         0.00357143 0.99342118 0.99342118 0.99233422 0.99233422\n",
      " 0.98224464 0.99233422 0.98290909 1.         0.99233422 0.99233422\n",
      " 0.99233422 0.99559959 0.99233422 0.72097706 0.99068433 0.99233422\n",
      " 0.         0.99342118 0.         0.99233422 0.99342118 0.99233422\n",
      " 0.99233422 0.99233422 0.99001987 0.9950472  0.99233422 0.99735099\n",
      " 0.99342118 0.99233422 0.99233422 0.99233422 0.99233422 0.\n",
      " 0.99133555 0.99342118 0.99342118 0.         0.99233422 0.\n",
      " 0.99233422 0.99233422 0.99668654 0.99233422 0.         0.\n",
      " 0.         0.99233422 0.99233422 0.99233422 0.99233422 0.\n",
      " 0.99233422 0.00740741 0.99233422 0.         0.99735099 0.99233422\n",
      " 0.99233422 0.         0.99342118 0.99233422 0.99233422 0.99735099\n",
      " 0.99233422 0.99342118 0.99342118 0.00357143 0.         0.00357143\n",
      " 0.99342118 0.99607019 0.97800221 0.99001987 0.99342118 0.99342118\n",
      " 0.99342118 0.98147357 0.99233422 1.         0.99559959 0.00740741\n",
      " 0.         0.99233422 0.         0.         0.00357143 0.99068433\n",
      " 0.         0.         0.99559959 0.99333333 0.98666667 0.97751987\n",
      " 0.99397357 0.00357143 0.99024859 0.99233422 0.99233422 0.99233422\n",
      " 0.99397357 0.04       0.99233422 0.99668654 0.99342118 0.\n",
      " 0.99233422 0.99342118 0.00357143 0.         0.99233422 0.99001987\n",
      " 0.99333333 0.99342118 0.99233422 0.99342118 0.         0.99233422\n",
      " 0.99233422 0.99233422 0.99342118 0.99342118 1.         0.98697928\n",
      " 0.         0.99342118 0.99233422 0.99233422 0.99233422 0.98485099\n",
      " 0.         0.99233422 0.99342118 0.00357143 0.99233422 0.99233422\n",
      " 0.99233422 0.9        0.99662258 0.97866667 0.98697928 0.99233422\n",
      " 0.99233422 0.99233422 0.99233422 0.99342118 0.00357143 0.99233422\n",
      " 0.         0.         0.99735099 0.00357143 0.99342118 0.99668654\n",
      " 0.99668654 0.99001987 0.9807523  0.         0.99668654 0.99233422\n",
      " 0.         0.99233422 0.         0.99068433 0.98485099 0.\n",
      " 0.99233422 0.         0.99233422 0.92466888 0.99233422 0.98418654\n",
      " 0.99342118 0.99233422 0.99233422 0.99233422 0.00740741 0.\n",
      " 0.97751987 0.99233422 0.98838053 0.99668654 0.99668654 0.99233422\n",
      " 0.         0.992      0.99342118 0.98697928 0.99735099 0.98433027\n",
      " 0.99233422 0.99233422 0.99735099 0.99001987 0.99233422 0.00357143\n",
      " 0.99233422 0.99735099 0.99024859 0.99001987 1.         0.99133555\n",
      " 0.99068433 0.         0.99342118 0.00740741 0.99233422 0.80740741\n",
      " 0.         0.99342118 0.99342118 0.98533333 0.99091304 0.99233422\n",
      " 0.         0.         0.99090909 0.00357143 0.99233422 0.\n",
      " 0.97800221 0.97800221 0.99233422 0.99233422 1.         0.00357143\n",
      " 0.99233422 0.99068433 0.00357143 0.99090909 0.         0.99233422\n",
      " 0.         0.         0.99133555 0.99342118 0.99233422 0.\n",
      " 0.99233422 0.93001987 0.99233422 0.99233422 0.99735099 0.99233422\n",
      " 0.98224464 0.99233422 0.99607019 0.99233422 0.99068433 0.\n",
      " 0.99233422 0.         0.22666667 0.99662258 0.99233422 0.99233422\n",
      " 0.99233422 0.91818433 0.98485099 0.99233422 0.99668654 0.98418654\n",
      " 0.99233422 0.99233422 0.99233422 0.99342118 0.99233422 0.99233422\n",
      " 0.99233422 0.99233422 0.99233422 0.99233422 0.99233422 0.99342118\n",
      " 0.98335321 0.98008785 0.98418654 0.99342118 0.00357143 0.\n",
      " 0.         0.         0.00740741 0.         0.99233422]\n",
      "<flaml.automl.model.ExtraTreesEstimator object at 0x00000250EED773A0>\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "print(automl.predict(X_train))\n",
    "# Export the best model\n",
    "print(automl.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7036c7-0f64-49b0-aba7-8a489d3a3d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
